%\begin{abstract}

%\end{abstract}

%\section{keywords}
%\begin{itemize}
%    \item Compare and swap
%    \item Non Blocking Data Structures
%    \item Asynchronous Data Structures
%    \item far memory data structures
%    \item optimisitc concurrency
%
%\section{intro}
\section{Forward Direction}

Alex has suggested that I look to the history of these algorithms and then use
that as a mechanism to discuss what has changed. This is more difficult than
just reading about dissagregated algorithms.


\section{History}

Disaggregated algorithms have the trait that they are one sided, and computed
over a network. All shared memory algorithms are one sided, but most of them
don't scale, however their techniques for dealing with mass concurrency are
relevant. Therefore the starting place for the history of this exam is
concurrency algorithms. NUMA systems take care to address the asymmetry of
memory accesses and divide operations into local and remote. This asymmetry
exists in the disaggregated context as well. However rather than any piece of
data having a \textit{local} master, all true data is remote. NUMA systems which
allow multiple remote accesses to the same data share this quality, and thus
their techniques are applicable. NUMA which defers to a local core for remote
requests does not apply here.

\subsection{key value stores} As such many prior techniques for key value stores
which take the network into account, such as RDMA optimizations, are still
applicable in the disaggregated context. Where the comparison between prior key
value stores and disaggregation ends is when two sided approaches like RPC or
two sided RDMA are utilized, especially when they perform the serialization. For
example, a common paradigms is to have the reads be performed async using one
sided operations, while the writes are serialized by a two sided operation. This
allows a CPU to perform the locking and execute multi step critical sections,
such as pointer chasing, with low latency.

\textbf{Pilaf~\cite{pilaf}} uses just such a mixture of one and two sided RDMA
operations. The key insight is that if a data structure is architects carefully,
the reads can be performed fully asynchronously by one sided RDMA reads which
dramatically reduces server side CPU usage and request latency. This technique
is common in concurrent algorithms, and is readily applicable here. One new
technique for remote memory structures which pilaf introduces is self verifying
data entries. These entries prevent data corruption during races. Incomplete
writes read by RDMA operations can be invalidated by the client. This is one
example of pushing the complexity of a data structure to the client side. Pilaf
does not extend to the disaggregated context directly as writes are handled by a
server side cpu which serializes requests, and processes CRC's. Their rational
is that client side atomic operations are slow, and brittle. 

\textbf{Farm~\cite{farm}}, similar to pilaf allows for async RDMA reads, and
uses one sided writes. However the writes are made to a buffer which is polled
by a server side CPU and later processed as a transaction. This technique in
some ways is a disaggregation applicable approach. At least the writes side of
the rx wheel uses disaggregated techniques. However as half of it's structures
manipulation requires server side processing it is not directly applicable. The
hash table Farm uses is applicable. They use Hopscotch Hashing~\cite{hopscotch}
which places data near keys. This allows a single read to get the values it
needs with a single round trip (as opposed to cuckoo hashing). This pushes the
complexity to the client. To my knowledge this if the first instance of pushing
read complexity to the client in any paper.

\textbf{Herd~\cite{herd}} Herd improves upon both Farm and pilaf by looking
closely at the RDMA specific behavior of KV stores. It considers the hash
algorithm carefully to bound the number of one sided reads. And uses RDMA writes
from server, rather than two sided operations to improve latency. Herds biggest
contribution is that it closely analyses RDMA performance and choses the verb
architecture closely. Herd does not apply greatly to disaggregated contexts as
is fully assumes server side CPUs. However, it's one sided benchmarks are valid
and valuable for disaggregated designers.

\textbf{fasst~\cite{faast}[\textcolor{red}{x}]} Fasst is Anujs first cut at RPC.
Similar to ERPC below, FASST argues heavily for the use of two sided verbs to
reduce the cost of reading. While this makes sense it's not super applicable.
Many of the observations are around batching, RDMA reliability, and scheduling.

\textbf{erpc~\cite{erpc}[\textcolor{red}{x}]} Erpc focues on common case RPC
operations, aiming to reduce drops, avoid pfc, and achieve 100MOPS. This is
almost entirely RDMA optimization. They note that using connections, the
scalability suffers. Around 4000 connections the read rate starts to drop.
Unfortunatly many of the implements for scaling such ad the RPC credit system
rely on server side participation. ERPC is therefore not a good source of info
for this RE.




\textbf{storm~\cite{storm}} Storm makes the argument that modern NICs have the
ability to scale well, in contrast to what has been stated in prior work.

\textbf{1RMA~\cite{1rma}}

\textbf{}

\subsection{NUMA}

\textbf{Flat Combining}
\textbf{Partitioned and replicated state}

\subsection{Lock Free Datastructures}

\subsection{RDMA}

Why use RDMA? There are a variety of ways to manipulate remote memory,
GMS~\cite{gms} would suggest that we coordinate using full networking stack,
however in the context of disaggregation we assume no remote CPU's to execute
the stack. We could connect via a cache coherent interconnect such a
Kona~\cite{kona} which makes use of cache lines. RDMA is a middle ground. The
RDMA spec gives a set of \textit{verbs} which are memory operations that the NIC
can execute. These verbs are similar in some respects to RPC calls, or simple
instructions. As long as an algorithm can be described using the RDMA calls it
can be executed on remote memory. The advantage to RDMA is that the operations
it supports are general. The downside is that some algorithms are difficult to
execute using one sided verbs. The prototypical example is pointer chaising
which requires a round trip for each resolved pointer.


%currently just a jotting area for ideas.
\section{Remote Memory Techniques}

The following are some far/disaggregated memory algorithm design techniques and
common issues.


\textbf{Steering} centralized in network compute can be used to steer and modify
requests when data races and conflicts occur. The downside is that this takes an
extra bit of in network processing, but it gets a big speed up. In order for a
middlebox to make the correct steering decision it requires all of the
information necessary to preserve the data invariants of the structure it is
steering into. This limitation demands that the data structure itself be well
conditioned for this property, and be able to deal with potential variance in
its structure.

A potential use for steering is also to perform in place updates while allowing
for reads. For example imagine updating a range on a data structure [x,y] the
first step of the operation could be to lock the region, then copy [x,y] to a
new location [x',y']. During the update reads are steered to the new [x',y'] and
the update is made in place on the old [x,y]. When the operation is complete the
steering stops and the update is atomically visible. This could be useful for
non-pointer based structures like hopscotch hashes~\cite{hopscotch}.

\textbf{Read Size} One tradeoff that can be made to accelerate writes is to make
them sloppy and push the complexity to the read. This adds a bit of computation
as the reader must compose the information they require from the unsorted read.
As a simple example imagine a multi processes sorted list. Each insert is
expensive if it must be globally coordinated. However it's cheap if each process
just maintains it's own locally sorted list. A call to \textit{max} which
returns the max globally could peek at the heads of each list, and then locally
compute the max from those. For $n$ processes this inflates the cost of the read
by $n$ in bandwidth however it removes the need for coordination. In general I
believe that this holds for all algorithms and data structures. Consider that any
structure can be represented by the set of operations performed on it. If each
write operation is append to a log in memory, with a globally orderable
timestamp then an arbitrary number of processes can have unorganized and full
write throughput, and push the composition of the data structure to each read,
where the entire log of each process is read, and executed locally in it's
entirety.

Algorithms like hopscotch hashing~\cite{hopscotch} (used in Farm~\cite{farm})
and Robinson hashing~\cite{robinhood} are designed to have good cache locality.
They are designed with an open addressing (closed hashing) scheme which states
that an index does not specify the location of the value. However, with these
hashes it will specify the neighborhood of the data, and provide good locality.
If this locality is designed to place relevant information in sequential memory
a scaled up read is appropriate to gain all meta, and data for the structure.
Perhaps more data structures, designed to be concurrent and cache optimized will
be good candidates for Disaggregated Memory.

\textbf{Write amplification mitigation} Remote structures sometimes require write
amplification. Consider a set of entries each guarded with a checksum for
integrity. An update to any entry requires the recomputation of the checksum,
and the rewrite of the continuous addresses in order to make the write in a
single round trip. Depending on the bucket size of the entries the write can
grow arbitrarily. Another technique is to make each entry have it's own checksum
for it's integrity, and to only update the bucket checksum when all items become
static.

\textbf{Precomputation} Some optimization are easy with preoccupation. Consider
Yizhous one level page table. Normally a walk would be required, however they
access the page with an O(1) hash. To do so they must avoid collisions. They
push the complexity to malloc which tries continuously to make an virtual address
allocation until it finds one without a hash collision.

\textbf{Optimistic Concurrency} Round trip times are extremely expensive.
Traditional locking requires a round trip for acquisition and for release making
the common case slow. Most far memory algorithms will benefit from optimistic
concurrency with the exception of write heavy ones which must access shared
data. I.e contentious algorithms. RDMA provides verbs such as CAS and FAA for
implementing optimistic concurrency.

\textbf{Caching} A variety of caching techniques reduce latency for far memory
application. Caching allows for a two fold benefit. The first is that the
distance between requests and the required data is less. Caching on a switch
removes the last hop in a rack, and on a NIC removes a PCIe round trip. The
dangers with caching is that the important caching data must be determined
somehow. Traditional caching techniques can lead to massive bandwidth inflation.
The second advantage of caching in network is centralization which can reduce
the communication between hosts.

\section{remote memory traps}

\textbf{Centralized Serialization} My approach uses a centralized serialization
point to alleviate coordination. However it has been shown that doing all the
work by a centralized core is slower than a carefully crafted concurrent
solution~\cite{near-memory-structs}. The insight here is that if we have a core
close to memory it can do things like pointer chasing faster than remote. They
demonstrate that PIMs are not sufficient to solve far memory problems. But it
will become bogged down at some point due to being centralized. That's when a
concurrent algorithm with less contention moves faster. The key thing about this
centralized point, is that it must have excess space above line rate with which
to coordinate. For example in ~\cite{fairnic}[Figure 2] we show that there is
some headroom for processing with x number of cores. This gap is the processing
gap that we can use to resolve contention.

\section{notes}

The papers~\cite{one-side-hash} and ~\cite{write-optimized-hash} are almost
identical. It's the same author, I'm just noting that the change between NUMA
and Far memory could use this as a case study.


Similar to the concept of a bandwidth delay product, in terms of remote memory
there is a latency operation product which is expanded by the time it takes for
an operation to complete. For example O(1) will have less active operations than
an O(2) data struture simply by the nature of the operations. Anything requiring
a read and a write to complete will take longer to complete due to the round
trip.

In the few concurrent papers I read there is the concept of performing a marking
on a data structure. For example this concurrent binary
tree~\cite{fast-concurrent-bin} marks edges to ensure that they do not get
modified in a way that breaks the tree. This marking is similar to placing a
lock. I think that similar to marking, there should be a way to check a data
structures in flight operations, if all the operations are known ahead of time,
and then serialize the requests based on that.

There is a good argument for algorithms between the two worlds of remote memory.
On one hand we have pure remote memory, one sided operations with zero
interference. These are unbelievably slow. And we can say from a straw man
perspective that they could use better algorithms. On the other hand we have
solutions like in~\cite{design-far-memory-struct,near-memory-structs}, which
suggest near memory computing with on board compute near memory. In this case
they also show that with the best near memory compute the old algorithms simply
do not work because the serialization gets overloaded.


%\section{review}  

%\section{conclusion}