% \section{Abstract}

\section{Introduction}

The imbalance between compute and memory capacity for data
center servers has only increased over the past decade.
Simultaneously networks have doubled in capacity at an
increasing rate and have integrated line rate
programmability into many commodity chips. These trends have
yielded various research proposals for attacking the
\textit{memory wall} with fast networks. Memory
disaggregation proposes the addition of pools of far-memory
attached over a network to clusters of compute machines,
with the promise that large shared pools will decrease
memory stranding and increase overall memory utilization.

Latency is the chief challenge in realizing practical memory
disaggregation. Low-latency interconnects such as RDMA have
intra-rack round trip latencies on the order of 1-2$\mu$s.
In contrast to local memory DRAM access times of 50-100ns
RDMA latency is a 20x overhead. This disparity prevents many
data structures, and system functions from being deployable
for remote memory. Existing systems have amortized the
network cost by porting slower subsystems i.e swap devices
~\cite{fastswap, legoos}\todo{add more}. 

These systems avoid a key feature of local memory
\textit{sharing}. Shared data structures in remote memory
can suffer abysmal performance when accessed concurrently.
Both traditional lock based and optimistically concurrent
data structures suffer abysmal performance when ported to
remote memory. Lock based algorithms have inflated critical
section sizes due to round trip delays, and optimistic
approaches suffer from stale caches under contention, and
the increased cost of reading to synchronize caches. In
general shared structures are hard to port to remote memory.

%% Todo this is my research statement work hard on it.
My work focuses on the design of algorithms and data
structures for remote memory.  A key insight of my work is
that the performance pitfalls of remote memory arise from
memory not having colocated compute to serialize operations.
We note that this pitfall is unnecessary because the network
implicitly serializes each operation during transit and
programable elements within the network can administrate
algorithm level serialization at line rate. With this
observation in mind we design shared far memory systems
using programmable devices such as NIC's and switches to
accelerate performance.

This research summary covers three projects which illustrate
how these devices can be used to improve optimistic data
structures, lock-based structures, and finally, shared
structures in general.  The first system \textit{Swordbox}
demonstrates that by caching the operations of an optimistic
algorithm on a programmable switch contention can be
entirely removed in network enabling up to 40x improvement
in throughput and 300x reduction in tail latency for
contested workloads. The second project \textit{RCuckoo}
shows how with careful algorithm design and the judicial use
of RDMA features a traditional local memory algorithm can be
ported to remote memory. Finally this work presents an
outline for general data structure design for remote memory
using a programmable switch to remove contention in the
general case.


\section{Optimistic Data Structures}

Optimistic concurrency enables high degrees of parallelism
in the absence of contention. At a high level concurrent
processes keep a cache of a shared structures state, and
then issue operations which succeed if the cache of read, or
written data is synchronized with the shared structure. When
memory latency, and contention are high synchronization is
difficult as reads are slower and more expensive quickly
causing local caches to become stale.

In our work \textit{Swordbox} we make the observation that
at rack-scale the top-of-rack (TOR) switch observes all
memory operations in serialized order. This de facto
serialization enables a programmable switch to resolve
conflicting updates to a shared data structure in flight.

In our prototype we resolve optimistic updates to a shared
link list which underpins Clover, a state of the art
disaggregated key value store~\cite{clover}. In Clover keys
are stored as a linked list, when clients update a key they
issue an append to the tail of the list. When keys are under
contention appends to the list frequently fail as the
clients issue appends to stale version of the lists' tail.

Figure~\ref{fig:swordbox} a) shows how the success rate of
clovers writes plummets as the number of concurrent clients
increase. When requests fail, clients must issue reads to
the list, find the tail, and execute the write again causing
throughput to drop, and tail latency to dramatically
increase. Swordbox fixes this performance pitfall by caching
the tail pointer of hot keys in a P4 program running on a
programmable TOR. When write pass through Swordbox it
updates it's tail pointer to the list.  When Swordbox sees
stale writes, i.e writes issued to old tail locations, it
rewrites the virtual address of the write to append it to
the tail of the list. This technique enables all contested
request to succeed while using only a small amount of switch
SRAM (64bits per key). Figure~\ref{fig:swordbox} b,c shows
both the throughput (32x) and latency (300x) improvements of
using Swordbox.

Swordbox demonstrates that optimistic algorithms can be
dramatically improved by caching contested variables in
network. Data structures which require updates to many
locations within a structure may be less amenable to
swordbox's design as the amount of state required to perform
the updates may be large. The future direction for this work
is to investigate data structure design which enables large
performance boosts through small amounts of in-network
caching.

\begin{figure*}[t!]
    \begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.9\linewidth]{fig/success_rate.pdf}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.9\linewidth]{fig/full_system_performance.pdf}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.9\linewidth]{fig/99th_latency.pdf}
    \end{subfigure}

    \caption{Swordbox performance benifits. \textbf{a)} The effect of contention on
    Clover's optimistic writes. \textbf{b)} System throughput
    for YCSB-A (50\% write). \textbf{c)} 99th percentile tail
    latency for different write percentages. Left: read
    latency. Right: write latency}
        
    \label{fig:swordbox}
\end{figure*}

\section{RCuckoo}

Not all data structures are amenable to oportunistic
updates. Some require extensive updates to a structure where
critical sections provided by traditional locking provides a
simple way to ensure consistency. Unfortunately locks port
poorly to remote memory. 
%%
Acquiring, and then releasing a lock requires at minimum two
round trips, a fact which not only increases the minimum
operation duration, but which sets a lower bound on critical
section length. A system with a 2us round trip time, and a
single global lock can only perform 500,000 operations per
second in the best case (meager numbers when considering that
decades old key value stores acheive 10's of millions of
requests per second ~\cite{herd}). Fine grained locking is a
requirement.  Unfortunately fine grained locking suffers from
a duel problem, deadlock free lock acquisition requires a
round trip per lock inflating operation time. 

With RDMA atomics many locks can be acquired in a single
operation, but only if they are within the word width of the
RDMA atomic (64 bits). Fine grained locking is therefore a
challenge of achieving good lock locality.

With this in mind we investigate porting cuckoo hashing to
remote memory. Cuckoo hashing has long been known to perform
well with RDMA reads as lookups must search at most 2
locations~\cite{pilaf, farm}. The complexity of cuckoo
hashing is pushed to writes which perform swaps randomly
throughout the table. 

We develop a new algorithm for locality aware cuckoo
hashing. Both hash locations of a key are \textit{likely} to
be near one another, rather than in random locations . Our
hash algorithm ensures that with high probability most pairs
of hash locations are close.  This technique increases the
probability that insertions to the table can be performance
with a high degree of locality within the hash table.

Locality provides many opportunities for network
optimizations. First because both updates and reads are
localized read can with high likelihood be performed with a
single RDMA read message. Second, because updates are
localized clients can quickly synchronize with the remote
table by issuing a single large RDMA read. These techniques
trade off bandwidth, which is usually plentiful in data
center networks for latency \todo{figure from rcuckoo that
shows the packet size latency tradeoff}

Locality also enables smarter search. Insertions with a
large number of table updates can be slow. Because updates
are localized we can use locality aware search (A*) to
quickly find the shortest insertion path.  

Finally and most importantly locality enables efficient
locking. Because updates are localized the locks for a given
set of table updates are likely to be close. We use this
fact, and RDMA masked compare and swap \todo{cite} to
acquire and release all required locks in two round trips in
the average case and found RTT at the 99th percentile.


\begin{figure*}[t!]
    \begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.9\linewidth]{fig/rdma_latency.pdf}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.9\linewidth]{fig/insertion_span.pdf}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.9\linewidth]{fig/search_dependence.pdf}
    \end{subfigure}

    \caption{RCuckoo algorithmic improvements. \textbf{a)}
    RDMA read latency vs message size. \textbf{b)}RCukoo
    hash locality. The span in table rows which insertions
    span using random hashing vs dependent hashing.
    \textbf{c)} 99th percentile round trips for insertions
    using random hashing vs dependent.}
    \label{fig:rcuckoo}
\end{figure*}



\section{Black Box Disaggregation}

A question remains about general data structure design for
remote memory. while high performanc can be achieved through
careful data structure design, is it possible enable general
data structure design for remote memory? My future research
direction aims to explore this question.

Prior work on NUMA systems has shown that highly concurrent
data structures can be desigend in geneal with a technqiue
known as \textit{node replication}. All operations to a
shared structure are made by appending the operations to a
shared log. Clients wishing to commit operations first read
the log, construct the dat structure locally, and then
append to the end of the log their new operation~\cite{bbd}.

This technique experiences the same contention issues as
clover in that multiple clients race for the tail of the
list, however on NUMA nodes the memory latency is low enough
that the technqiue scales well enough for practical
purposes. 

We propose a similar technqiue for remote memory, with the
addition that switch, or programmable nic close to memory be
used to remove contention from the shared list. The switch
or NIC can provide two fold services. The first of which is
to steer appends to the shared list to the end of the list
enabling the success of each client request. Second when
clients read the shared log, and are unaware of the tail
location, the switch can inflate the size of the reads in
flight by modifying the RDMA read request. 

Using these techniques black box disaggregation can enable
generic high performance shared data structures for remote
memory.

\todo{include a diagram of the system}.

\section{Conclusion}
