\section{Introduction}
\label{sec:intro}

This document is broken into sections for the purpose of partioning research
concerns. Section~\ref{sec:issues} is a laundry list of design points in the
DDC space. The intent is to build a cross paper set of arguments and references
for each design point. Section~\ref{sec:related} summerizes papers at a high
level, and contains ideas relevent to those papers.
Section~\ref{sec:experiment} contains potential back of the envelope and large
scale experiments which could be run to either validate or promote further
research into an idea. Section~\ref{sec:arguments} is scratch space for
rehetorical arguments. finally Section~\ref{sec:reading} contains citations to
related work and tags for the level of detail at which they have been read.

\section{DDC design issues}
\label{sec:issues}

\subsection{Coherence}

As CPU's grow to thousands, and potentially hundreds of thousands of cores coherency becomes extremely expensive. A tiered approach may be required where some memory is strongly coherent and other memory is managed by lightweight distributed protocols~\cite{189914}

\subsection{Failure}

Disks fail more often than expected~\cite{Schroeder:2007:DFR:1267903.1267904}.
Perhaps the same could happen to main memory at scale? What would this look
like for NVM? What techniques are necessary to protect from failures of this
particular frequency.

\section{Related Work}
\label{sec:related}

\textbf{Beyond Processor Centric Architectures}~\cite{189914} \\
%%
This HP Machine position paper argues for a memory centric computing
architecture where NVMe sits at the middle of many CPU banks, and where each
bank manages a large amount of private by volatile main memory. It asserts
that cache coherence is impossible at the scale of 100,000 cores and thus new
memory models are required. Claims also include high radix optical switching
will make NVM accessible at ultra low latencies. Key to their architectural
proposal is that there will be a \textit{shared something} model where each CPU
has it's own DRAm cache, and there is a large NVM shared resource behind all of
it. Essentially they are just replacing main memory with NVM and using DRAM as
cache with the difference that main is also persistent.

\idea{There is potentially an argument against this if the timings for NVMe are
within a comparable time to DRAM. The argument is why page out to DRAM if it's
not the point of consistency anyways? Just use L3 or start calling DRAM L4 and
NVM main.}

\idea{Given that memory is so far apart in this architecture special techniques
    may be needed for IPC. It's a given that writing to shared memory will be
    expensive and copies will be huge, perhaps IPC will all have to be
    implemented using pointer passing techniques. Another thought is that there
may be DRAM which is closer to CPU's from a coherence perspective. It may make
more sense for CPUs which some sort of \textit{Coherence Index} to share with
one another.}

\idea{If memory failures must occur, why not treat all function calls as RPC.
Any lost remote memory is then just an RPC failure, the semantics of which are
dealt with by the calling application. Could existing applications be modified
to these semantics automatically?}

\textbf{Welcome to Zombieland}\cite{zombieland}
This paper argues for a disaggregated memory management scheme which decouples CPU's from memory via power supply. The idea is thus, CPU's do not run all the time, and thereby use too much memory given that they have a static allocation of memory up front. The proposed design is to put the CPU into a zombie state when it is not being used and then patch off of it using RDMA from other active CPU's

\idea{While this idea is mainly about power consumption it may yield an
interesting result, mainly a zombie memory sharing system whereby busy cores
can "eat" the memory of a zombie process. Perhaps when the busy cores require
memory they can swap the zombie cores out to disk.}

\textbf{Disk failure in the real
world}~\cite{Schroeder:2007:DFR:1267903.1267904} Disk failures are often far
higher than expected given data sheets. In some cases expected disk failure
rates of 0.8\% can average as high as 3-5\% with some applications ranging up to
13\%.

\textbf{CacheCloud}~\cite{cachecloud} The Network at 400GB is producing packets faster than DRAM can consume them, because of this L3 cache is proposed as an alternative, rather than go to DRAM just wire NIC's directly to L3. The argument is essentially that getting the packets closer to the compute faster will get them processed faster (intuitive, but it assumes no resource really need to be used).

What should the memory hierarchy in a DDC look like. Once could imagine that if some CPU's can send messages to their L3 caches directly then they really are just sharing caches. This is identical to a NUMA system.

\idea{Design an allocation strategy which dynamically builds NUMA systems. i.e.
CPU caches directly message one another, and then use backing DRAM as a
consistency mechanism. If the DRAM is really the latency hog and not the network
then partitioning a system by these parameters may be beneficial.}

\textbf{LegoOS}~\cite{legoos} Argues that each network attached component be an
individual fault domain. I think that arguing for configurable fault domains is
superior. Further they argue for a single controller for each component which
requires a lot of overhead. They argue to \textit{virtualize}, I think I can
make an argument for raw hardware. Loose coupling is pushed hard, a counter
argument for tight coupling with an "amortized" failure cost could be far more
compelling. Again I like the idea of \textit{binding} a failure domain.

LegoOS uses an ExCache, which is just a large ammount of DRAM associated with
each processor to give them good locality. This is really a violation of what
DDC will be in the future and deserves an explicit argument against it as
being regressive, and false. At the same time an argument for the necessity of
ExCache could be made if all of the backing memory is NVM. In which case
caching a bunch of cores with a piece of fast DRAM makes total sense so that
faults can be isolated. The big problem is that ExCache is not shared! They
also seem to cut out TLB's because the idea of swapping virtual address spaces
is not fast.

\idea{It seems that one of the core concepts of DDC is the decoupling of
physical memory and CPU cores. The ephemeral boundary typically exists at the L3
cache level. Perhaps there is an argument to be made for fixing a chunk of
physical memory to a set of CPU's, letting them coordinate over L3 cache, and
then writeback to main memory when the time comes. the consistency level could
be maintained on a per CPU allocation boundary.}

\idea{Developers do not understand fault domains at the intra process level.
However, for the savvy dev it may be nice to give them some form of fault
tolerant knowledge. For example we could develop a signal (partial fault) which
the DDC OS raises and which a process can use to resolve issues of partial
failure before dying. These would all have to be mapped into the OS and be
issued when the processes dies.}

\textbf{Clover~\cite{clover}}

Persistant memory is explored in the context of a key value store. The point
of this paper is to explore the design space of how to use and access
persistent memory (Non volitile main memory). The discussion is in the
context of where the data and metadata should be stored. The authors explore
a number of design decisions and explore protocols and archetectures with
various tradeoffs. See figure 1 of the paper. The authors draw a distinction
between active and passive memory. Active memory has a CPU like component on
the memory controller which controlls access. Passive memory is 100 percent
controlled remotely. In this case by an RDMA nic running at 100gbps. The
authors explore where to place the controller. They eventually come to the
design that a metadata server should exist seperate from the CPU's and
memory. It is accessed only to determine where the appropriate memory for a
given key is. Any attempt to put the metadata server between the CPU and the
Memory leads to performance degredation, but can improve write contention. A
large component of the solution space here is to allow reads and writes to
execute concurrently in the normal case. The autors propose a chain like data
structure with chaches of pointers to the most recent data which allows
processers to bypass the meta data server unless their information is out of
date. 

Even with thier best efforts they get memory latencies of around 1us. This is
super good, but far from the access latency of main memory. Finally they use
values of size 1KB in order to saturate throughput. So the idea that
persistant memory is going to be used for byte level access is not realistic
as of yet.

\idea{Given the current landscape using persistant memory for paging out
might be the correct decision. This would allow huge banks of persistant
pages, which could be accessed like a key value store. It would also allow a
program to have a logical point in its execution at which it's state is
persited. Users of such a system would still need to run a \textit{Persist}
instruction to create a snapshot. But the cost of performing the snapshot
would be limited to the working set of main memory.}

\textbf{Storm ~\cite{storm}} RDMA perfomance and scaling paper. They propose
a transactional later for improving one sided operations on RDMA. They also
do work to demonstrate that one sided operations are better suited to
connetion based protocols rather than connectionless (UDP) with one sided
operations. This is in heavy contrast to prior work.

Shu-Ting pointed out that this is really the logical successer to the Farm,
Herd, eRPC work. With this being the version for the latest version of
Melanox RDMA enabled NICs. I believe that the key takeaways from this work
are the absolute values from the measurements taken. It is a snapshot of the time.

Also the three takeaways of persistant connections, scaling to the rack
level, and requirement for imbedded data structures are key. The data
structure / pointer chaising aspect is something that I had pointed out to
alex as a potential succession to the work we were doing on smartNIC's.

\idea{I could take each of the papers in the progression that Shu-Ting
pointed out and build a table / plot ofthe competing technologies. The
interesting takeawyas would be the projections of where the technology is
going. For example are we processing requests at an ever increasing rate? Is
there a point in time at which we would expect certian traditional approaches
to start applying again for disaggregated techonologies?}

\textbf{LITE ~\cite{lite}} Low level abstractions for a kernel which allow
for high perfomance RDMA. Key here is that they avoid copy by using physical
addresses for user space buffers. They provide a kernel level abstraction for
better integrating data center aplications with RDMA.

\idea{I think that what this paper is getting at is that remote resources are
difficult to interact with. My thought here is not only should a SmartNIC
allow a programmer to perform API calls on remote data structures, that
should be it's explicit purpose. I.E a smartnic should be a semantic gateway
to remote structures. A programmer provides an API to a data structure such
as a B-Tree, which is then offloaded onto the NIC. In cases where access is
local, the B-Tree is accessed locally. When Remote the NIC amortizes the
round trips by opaquely performing the operations in bulk. This will reduce
overheads in the form of pointer chaising when many round trips are
pqincurred.}


\section{Experimental Questions}
\label{sec:experiment}

What is the mean time to failure for a byte on DRAM, FLASH, NVME? Perform a
back of the envelope calculation as to the expected mean time to failure on a
rack descried at the beginning of section 2 of~\cite{189914}. How often do
failures occur in a data centre filled with such racks? How much redundancy is
justified?

DDC represents a distributed OS that is far tightly coupled than prior WAN and
LAN approaches.

What view should a user have of a DDC? Should they know that individual
components are connected over the network? Should all devices be mounted? How
should developers reason about latency given a single PC view of a DDC?
Arguably if everything is connected over the network latency should be the
same, just slower.

LegoOS~\cite{legoos} argues for heterogeneity in the DDC. This is an argument
which should be countered. A key questions is if DDC at a rack scale will be
homogeneous. I think that individual blades will probably be manufactured to
have the same silicon on them. They cite TPU, which has dedicated racks.

What is the correct failure domain? if I decide that a process is the correct
failure domain, what do I do when some of the process disappears? \idea{Could it
be possible to persist a process on a context switch? If NVM is a thing perhaps
the act of migrating a process out of a processor could be a persistent
operation. The question is what to do with side effects. A process would need
to be persisted every time it made an action which could have a causal
effect.}

A quick survey of DC applications and their fault tolerance requirements would
be a great motivator for the binding tactics I'm arguing for i.e. How many
datacenters run Cassandra to manage their fault tolerance, how many are
running ETCD? what are the protocols for any one of those processes failing in
the first place? Treating a single rack as a partially independent failure
domain is silly, lets argue for the number of failures which would effect an
entire rack, and then talk about the ones that compromise some sub component of
the rack. If the former highly outweighs the later, then it still falls to
software to deal with fault tolerance and persistence and not the OS.

Before making any large scale design decision make sure to REALLY understand
the benefits of splitkernel / multi kernel design vs monolithic.

\idea{(Sleepy) Use Kademlia as either virtual memory or as page table lookups}
How should memory be accessed in a distributed rack? Lego's would say to use
virtual memory~\cite{legoos} but that does not take into account that failures
can and do occur. A page table is really just an address prefix, and a
traversal of the page table is just an additional lookup for an extension of
the prefix. Kademlia looks up each bit at a time, but it can be excelerated
using buckets where groups of a hashes ID are bunched together. Assuming that
there are processes running on the disaggregated rack when they want to alloc
more memory or access a file, they perform a Kademlia \textit{FIND\_NODE}
operation. If they do not find the file (potentially because it has failed),
the lookup fails and the region of memory cannot be mapped into the processes
memory region. These semantics assume copy on read. The memory model would be
closer to a segmented model, sort of like Multics where the address space of
any given process would just be a series of memory mapped regions. This has the
advantage that processes operate in a single failure domain while still being
able to share.

\section{Arguments}
\label{sec:arguments}

Current servers share failure domains, why then should disaggregated servers, which
in their manifestation appear to act as a single machine attempt to deal with
fault tolerance at the kernel level? We argue that performance not fault
tolerance should be the aim of a server, and that fault tolerance is an end to
end concern which should be handled by software running on disaggregated
machines rather than by the kernel. We propose the concept of \textit{binding
failure domains} resources allocated per bind fate share. We propose multilevel
binding at the process, VM and OS level. What then is a distributed OS to do in
the face of a partial failure? The key idea here is that while a process might fail, an OS is eternal (at least in the face of a partial failure, and given that the OS itself is protected.

There is an argument to be made that as long as a process does not have
external effect's than it can be protected by the OS. However, if it makes
external calls, networking or writes, then it must protect itself from itself.
The argument is one of end to end: Any process which admits external effects,
and wishes to operate under the assumption that those external effects are
respected but account for them themselves. No operating system in the past has
ever made guarantees to software which sent messages beyond it's boundaries, or
which passed messages over the network, or to the disk.

\section{Reading}
\label{sec:reading}
\begin{itemize}
    \item{Beyond process-centric operating systems}~\cite{189914}\rpaper{}
    \item{Dark Packets}\rabstract{}
    \item{CacheCloud}~\cite{cachecloud}\rintro{+}
    \item{It's time for low latency (osterhouse)}
    \item{LegoOS}\cite{legoos}\rpaper{}
    \item {Clover}\cite{clover}\rpaper{}
    \item{fault isolation (Amandas work)}\rintro{}
    \item{Decibel - Mihir}\rabstract{}
    \item{Popcorn Linux}
    \item{Helios - Heterogeneous multiprocessing with satellite kernels}~\cite{helios}\rabstract{}
    \item{Welcome to Zombieland}~\cite{zombieland}\rintro{}
    \item{Disk failure in the real world}~\cite{Schroeder:2007:DFR:1267903.1267904}\rabstract{}
    \item{The Multikernel: A new OS Architecture for Scalable Multicore Systems}~\cite{the-multikernel}\rabstract{}

\end{itemize}
